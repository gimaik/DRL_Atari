{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting Global Parameters\n",
    "\n",
    "class PG_Agent(): \n",
    "    \n",
    "    def __init__(self, num_state, num_action, params):\n",
    "        \n",
    "        self.params =  params\n",
    "        self.num_state = num_state\n",
    "        self.num_action = num_action\n",
    "        self.steps = 0        \n",
    "        self.gamma = params['gamma']   \n",
    "        self.learning_rate = params['lr']\n",
    "        self.load_model = params['load_model']\n",
    "        \n",
    "        self.model = self._create_model(load = self.load_model)\n",
    "        self.states = []\n",
    "        self.gradients = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "\n",
    "        \n",
    "    # Model OK    \n",
    "    def _create_model(self, load = False):\n",
    "                \n",
    "        if not load:\n",
    "            model =  Sequential()\n",
    "\n",
    "            model.add(Conv2D(input_shape = self.num_state, filters = 32, \n",
    "                             kernel_size = 4, strides=2, \n",
    "                             activation = 'relu'))\n",
    "\n",
    "            model.add(Conv2D(filters = 64, kernel_size = 4, strides=2, \n",
    "                             activation = 'relu'))\n",
    "\n",
    "            model.add(Conv2D(filters = 128, kernel_size = 4, strides=2, \n",
    "                             activation = 'relu'))            \n",
    "            \n",
    "            model.add(Flatten()) \n",
    "            model.add(Dense(units=200, kernel_initializer='glorot_normal',\n",
    "                            activation = 'relu')) \n",
    "            model.add(Dense(self.num_action, activation ='softmax'))              \n",
    "\n",
    "            optimizer = Adam(lr = self.learning_rate)\n",
    "            model.compile(optimizer = optimizer, loss = 'categorical_crossentropy')\n",
    "\n",
    "            print(\"Model constructed...\", end =\"\\r\", flush=True)\n",
    "        else: \n",
    "            model = load_model('pg-atari.h5')\n",
    "            print(\"Model loaded...\", end =\"\\r\", flush=True)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Check OK\n",
    "    def predict(self, state):        \n",
    "        if len(state.shape) == 3:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def discounted_rewards(self, rewards):\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in range(rewards.size, 0, -1):\n",
    "            if rewards[t-1] !=0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + rewards[t-1]\n",
    "            discounted_r[t-1] = running_add\n",
    "            \n",
    "        discounted_r = (discounted_r -np.mean(discounted_r))/np.std(discounted_r)\n",
    "        \n",
    "        return discounted_r\n",
    "\n",
    "\n",
    "    # Check OK\n",
    "    def observe(self, state, action, prob, reward):\n",
    "        y = np.zeros((self.num_action))\n",
    "        y[action] = 1\n",
    "        self.gradients.append(y - prob)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.steps +=1 \n",
    "             \n",
    "    # Check OK    \n",
    "    def act(self, state): \n",
    "        prob = self.predict(state)\n",
    "        self.probs.append(prob)\n",
    "        prob /= np.sum(prob)\n",
    "        prob = np.squeeze(prob)\n",
    "        action = np.random.choice(self.num_action, 1, p=prob)[0]\n",
    "        return prob, action\n",
    "    \n",
    "    def train(self):\n",
    "        gradients = np.vstack(self.gradients)\n",
    "        rewards = self.discounted_rewards(np.vstack(self.rewards))\n",
    "        gradients *=rewards\n",
    "        \n",
    "        X = np.squeeze(np.vstack([self.states]))\n",
    "        Y = self.learning_rate * np.squeeze(np.vstack([gradients])) # np.squeeze(np.vstack(self.probs)) + \n",
    "        \n",
    "        self.model.train_on_batch(X,Y)\n",
    "        #self.states, self.probs, self.gradients, self.rewards = [], [], [], []\n",
    "\n",
    "\n",
    "class input_pipeline():    \n",
    "    \n",
    "    def __init__(self, state):       \n",
    "        self.history_length = 4        \n",
    "        self.input_x=[]\n",
    "        self.input_x = [self._preprocess(state) for i in range(self.history_length)]\n",
    "        self.x = np.moveaxis(np.array(self.input_x), 0, -1) \n",
    "\n",
    "    def _preprocess(self, state):    \n",
    "        state = state[33:196,:, 0]\n",
    "        state = state[::2, ::2]\n",
    "        state[state == 109] = 0\n",
    "        state[state == 144] = 0\n",
    "        state[state != 0] = 1 \n",
    "        return state\n",
    "    \n",
    "    def update(self, state):\n",
    "        self.input_x.pop(0)\n",
    "        self.input_x.append(self._preprocess(state))\n",
    "        self.x = np.moveaxis(np.array(self.input_x), 0, -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(train=True):\n",
    "    \n",
    "    env = gym.make('PongDeterministic-v4')\n",
    "    env = gym.wrappers.Monitor(env, './tmp/pong-1', force=True)\n",
    "    num_state = env.observation_space.shape    \n",
    "    num_action = env.action_space.n\n",
    "    \n",
    "    agent = PG_Agent((82,80,4), num_action, params) \n",
    "    loss, mean_av, render = [], [], False    \n",
    "    \n",
    "    for episode in range(N_EPISODE):        \n",
    "        state, step, total_reward, done, render = env.reset(), 0, 0, False, False        \n",
    "        pipeline = input_pipeline(state)\n",
    "       \n",
    "        if episode >= 1000:\n",
    "            render = True\n",
    "        \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                \n",
    "            state = pipeline.x\n",
    "            prob, action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)  \n",
    "            agent.observe(state, action, prob, reward)\n",
    "            pipeline.update(next_state)\n",
    "            total_reward += reward\n",
    "            step +=1\n",
    "            \n",
    "            if done:\n",
    "                agent.train()\n",
    "        \n",
    "                if episode % 10==0:\n",
    "                    agent.model.save('pg-atari.h5')\n",
    "\n",
    "        \n",
    "        print('Episode: {}/{}, Step: {}, Iteration: {}, Reward: {}'\n",
    "              .format(episode+1, N_EPISODE, step, agent.steps, total_reward), \n",
    "              end = '\\r') \n",
    "        \n",
    "    return np.array(loss), np.array(mean_av)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['gamma'] = 0.99\n",
    "params['lr'] = 1e-3\n",
    "params['load_model'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = np.array(range(150000))\n",
    "# y = params['eps_min'] + (params['eps_max']-params['eps_min'])*np.exp(-params['decay']*x)\n",
    "# plt.plot(y)\n",
    "# plt.title('$\\epsilon$ decay rate')\n",
    "# plt.ylabel('epsilon')\n",
    "# plt.xlabel('iteration')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_EPISODE = 1000000\n",
    "loss, mean_q = main(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('iteration')\n",
    "# plt.ylabel('loss')\n",
    "# plt.plot(loss)\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(mean_q)\n",
    "# plt.title('Mean Q-Values')\n",
    "# plt.xlabel('iteration')\n",
    "# plt.ylabel('mean Q value')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
