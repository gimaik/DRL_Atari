{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import rescale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Global Parameters\n",
    "\n",
    "params = {}\n",
    "params['gamma'] = 0.999\n",
    "params['eps_min'] = 0.10\n",
    "params['eps_max'] = 0.50\n",
    "params['decay'] = 1e-4\n",
    "params['batch_size'] = 32\n",
    "params['lr'] = 2e-4\n",
    "params['capacity'] = 10000\n",
    "\n",
    "\n",
    "class DQN_Agent():    \n",
    "    \n",
    "    \"\"\"   \n",
    "    Attributes:\n",
    "    num_state:  number of states under state space\n",
    "    num_action: number of actions under the action space\n",
    "    gamma:  discount factor   \n",
    "    params: parameters for Q-network    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_state, num_action, params):\n",
    "        \n",
    "        self.params =  params\n",
    "        self.num_state = num_state\n",
    "        self.num_action = num_action\n",
    "        self.steps = 0\n",
    "        \n",
    "        self.gamma = params['gamma']   \n",
    "        self.eps_min = params['eps_min']\n",
    "        self.eps_max = params['eps_max']\n",
    "        self.eps = params['eps_max']\n",
    "        self.decay = params['decay']\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.capacity = params['capacity']\n",
    "        self.model = self._create_model()\n",
    "        self.memory=[]\n",
    "        \n",
    "    # Model OK    \n",
    "    def _create_model(self):\n",
    "        model =  Sequential()\n",
    "        \n",
    "        model.add(Conv2D(input_shape = self.num_state, filters = 48, kernel_size = 8, strides=2, \n",
    "                         activation = 'relu'))\n",
    "        \n",
    "        model.add(Conv2D(filters = 96, kernel_size = 4, strides=2, \n",
    "                         activation = 'relu'))\n",
    "        \n",
    "        model.add(Flatten()) \n",
    "        model.add(Dense(units=256, kernel_initializer='glorot_normal')) \n",
    "        model.add(Dense(self.num_action))              \n",
    "                 \n",
    "        optimizer = Adam()\n",
    "        model.compile(optimizer = optimizer, loss = 'mse')\n",
    "        \n",
    "        print(\"Model constructed...\", end =\"\\r\", flush=True)\n",
    "                 \n",
    "        return model   \n",
    "    \n",
    "    # Check OK\n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    # Check OK\n",
    "    def predict_one(self, state):        \n",
    "        state = np.expand_dims(state, axis=0)        \n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    # Check OK\n",
    "    def observe(self, state, action, next_state, reward, done):\n",
    "        self.memory.append((state, action, next_state, reward, done))    \n",
    "        \n",
    "        if len(self.memory) > self.capacity:\n",
    "            self.memory.pop(0)\n",
    "            \n",
    "        self.eps = self.eps_min + (self.eps_max - self.eps_min) * math.exp(-self.steps * self.decay)        \n",
    "        self.steps +=1        \n",
    "        \n",
    "    # Check OK    \n",
    "    def act(self, state):      \n",
    "        if np.random.rand() < self.eps:\n",
    "            action = np.random.choice(self.num_action)\n",
    "        else:            \n",
    "            action = np.argmax(self.predict_one(state))                       \n",
    "        return action  \n",
    "       \n",
    "    def replay(self):\n",
    "        batch = np.array(random.sample(self.memory, min(self.batch_size, len(self.memory))))\n",
    "        batch_len = len(batch)\n",
    "        \n",
    "        states = np.array([batch[i][0] for i in range(batch_len)])       \n",
    "        next_states = np.array([(np.zeros(self.num_state) if episode[2] is None \n",
    "                                 else episode[2]) for episode in batch])\n",
    "        action = np.array([batch[i][1] for i in range(batch_len)])\n",
    "        reward = np.array([batch[i][3] for i in range(batch_len)])\n",
    "        done = np.array([batch[i][4] for i in range(batch_len)])\n",
    "                     \n",
    "        q = self.predict(states)\n",
    "        q_new = self.predict(next_states)\n",
    "        \n",
    "        y = np.zeros((batch_len, self.num_action))        \n",
    "        \n",
    "        for i in range(batch_len):\n",
    "            target = q[i]\n",
    "            if done[i]:\n",
    "                target[action] = reward[i]\n",
    "            else:\n",
    "                target[action] = reward[i] + self.gamma*np.max(q_new[i]) \n",
    "            y[i] = target\n",
    "            \n",
    "        history = self.model.fit(states, y, verbose = 0, epochs=1) \n",
    "        mean_action_value = np.mean(np.mean(q))\n",
    "        \n",
    "        return history, mean_action_value\n",
    "    \n",
    "           \n",
    "    def train(self, state, action, next_state, reward, done):        \n",
    "        new_q = reward if done else reward +  self.gamma * np.max(self.predict_one(next_state))\n",
    "        old_q = self.predict_one(state)\n",
    "        old_q[0][action] = new_q\n",
    "        state = np.expand_dims(state, axis = 0)        \n",
    "        history = self.model.fit(state, new_q, verbose = 0, epochs=1)        \n",
    "        return history\n",
    "    \n",
    "\n",
    "class input_pipeline():    \n",
    "    \n",
    "    def __init__(self, state):       \n",
    "        self.history_length = 4        \n",
    "        self.input_x=[]\n",
    "        self.input_x = [self._preprocess(state) for i in range(4)]\n",
    "        self.x = np.moveaxis(np.array(self.input_x), 0, -1) \n",
    "\n",
    "    def _preprocess(self, state):    \n",
    "        state = state[30:195,7:154, :]\n",
    "        state = np.mean(state, axis = 2)\n",
    "        state = state[::2, ::2]\n",
    "        state[state==162] = 80\n",
    "        state[state==180] = 90\n",
    "        state[state==198] = 100\n",
    "        state[state==200] = 110   \n",
    "        return state / 255\n",
    "    \n",
    "    def update_state(self, state):\n",
    "        self.input_x.pop(0)\n",
    "        self.input_x.append(self._preprocess(state))\n",
    "        self.x = np.moveaxis(np.array(self.input_x), 0, -1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    env = gym.make('Breakout-v0')\n",
    "    num_state = env.observation_space.shape    \n",
    "    num_action = env.action_space.n\n",
    "    \n",
    "    agent = DQN_Agent((83,74,4), num_action, params) \n",
    "    \n",
    "    \n",
    "    loss, mean_av, render = [], [], False\n",
    "    \n",
    "    \n",
    "    for episode in range(N_EPISODE):\n",
    "        \n",
    "        state, step, done = env.reset(), 0, False\n",
    "        pipeline = input_pipeline(state)\n",
    "        x = np.zeros_like(pipeline.x)\n",
    "        \n",
    "        \n",
    "        if episode > 0:\n",
    "            render = True\n",
    "        \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                \n",
    "            action = agent.act(pipeline.x)            \n",
    "            next_state, reward, done, info = env.step(action)            \n",
    "            \n",
    "            x = pipeline.x\n",
    "            pipeline.update_state(next_state)\n",
    "            x_new = pipeline.x  \n",
    "            \n",
    "\n",
    "            agent.observe(x, action, x_new, reward, done)\n",
    "            history, mean_action_value = agent.replay()\n",
    "            \n",
    "            \n",
    "            \n",
    "#             loss.append(history.history['loss'])\n",
    "#             mean_av.append(mean_action_value)\n",
    "           \n",
    "            step +=1\n",
    "        \n",
    "\n",
    "        if episode % 10==0:\n",
    "            agent.model.save('dqn.h5')    \n",
    "            \n",
    "        print('Episode {}/{}, Step:{}'.format(episode+1, N_EPISODE, step), end = '\\r', flush = True) \n",
    "        prev_x = None\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-20 09:44:33,554] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 146/10000, Step:386\r"
     ]
    }
   ],
   "source": [
    "N_EPISODE = 10000\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
